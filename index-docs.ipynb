{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "import meerkat as mk\n",
    "from rich import print\n",
    "\n",
    "cwd = os.getcwd()\n",
    "mk.gui.start(api_port=5032, frontend_port=8010, skip_build=True)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'meerkat'...\n"
     ]
    }
   ],
   "source": [
    "REPO = \"https://github.com/hazyresearch/meerkat\"\n",
    "subprocess.run([\"git\", \"clone\", REPO])\n",
    "\n",
    "DIR = \"./meerkat/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Git repository object\n",
    "repo = git.Repo(DIR)\n",
    "\n",
    "# Get the list of files of interest\n",
    "files = repo.git.ls_files(\"--exclude-standard\", \"--cached\", \"--modified\", \"--other\").splitlines()\n",
    "paths = [os.path.join(DIR, f) for f in files]\n",
    "\n",
    "# For each file, get {'filename', 'len', 'extension'}\n",
    "files = [{'filename': f, 'nchars': os.path.getsize(f), 'extension': pathlib.Path(f).suffix} for f in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [00:01<00:00, 455.87it/s]\n",
      "/Users/krandiash/opt/anaconda3/envs/talkdoc/lib/python3.9/site-packages/meerkat/ops/map.py:260: UserWarning: Non-default argument 'row' does not have a corresponding column in the DataFrame. If your function expects a full DataFrame row, pass ``inputs='row'`` to ``map``. Otherwise, please provide an `inputs` mapping or pass a lambda function with a different signature. See map documentation for more details.\n",
      "  warnings.warn(\n",
      "100%|██████████| 735/735 [00:04<00:00, 174.52it/s]\n"
     ]
    }
   ],
   "source": [
    "project = mk.DataFrame(files)\n",
    "# Add a column that contains the actual file\n",
    "project['files'] = mk.files(project['filename'], type=\"code\")\n",
    "\n",
    "# Go through all the files in order, keep track of failed loads and their extensions\n",
    "failed_extensions = set()\n",
    "for i in range(len(project)):\n",
    "    try:\n",
    "        project[\"files\"][i]()\n",
    "    except Exception:\n",
    "        failed_extensions.add(project[\"extension\"][i])\n",
    "\n",
    "# Make a list of image and pdf file extensions\n",
    "remove_extensions = [\n",
    "    \".png\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".gif\",\n",
    "    \".svg\",\n",
    "    \".pdf\",\n",
    "    \".ico\",\n",
    "    \"\",\n",
    "    \".dia\",\n",
    "    \".odg\",\n",
    "    \".pkl\",\n",
    "    \".npz\",\n",
    "    \".fits\",\n",
    "    \".mod\",\n",
    "    \".swg\",\n",
    "    \".star\",\n",
    "    \".npy\",\n",
    "] + list(failed_extensions)\n",
    "# remove_extensions = set(remove_extensions) # this doesn't work?!\n",
    "\n",
    "# Exclude files that are images or pdfs\n",
    "project = project.filter(lambda extension: extension not in remove_extensions, pbar=True)\n",
    "\n",
    "# Exclude files that are empty\n",
    "project = project.filter(lambda row: row['nchars'] > 0, materialize=False, pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Tokens: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1852266</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Tokens: \u001b[1;36m1852266\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tiktoken\n",
    "get_token_count = lambda files: [len(e) for e in tiktoken.get_encoding(\"gpt2\").encode_batch(files)]\n",
    "project['ntokens'] = project['files'].map(get_token_count, batch_size=128, is_batched_fn=True, pbar=True)\n",
    "print(\"Total Tokens: {}\".format(sum(project['ntokens'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://localhost:8010/?id=Table4b4d5161-e8d9-4ef6-a675-f7f2bcad2a97\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fafb8286460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project.create_primary_key(\"file_id\")\n",
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 36.92it/s]\n",
      "/Users/krandiash/Desktop/workspace/projects/meerkat/meerkat/ops/map.py:260: UserWarning: Non-default argument 'row' does not have a corresponding column in the DataFrame. If your function expects a full DataFrame row, pass ``inputs='row'`` to ``map``. Otherwise, please provide an `inputs` mapping or pass a lambda function with a different signature. See map documentation for more details.\n",
      "  warnings.warn(\n",
      "100%|██████████| 695/695 [00:11<00:00, 60.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Callable, List\n",
    "\n",
    "def explode(df: mk.DataFrame, chunk_col: str, chunker: Callable, batch_size: int = 1) -> mk.DataFrame:\n",
    "    \"\"\"Chunk each row of a DataFrame into multiple rows, and concatenate the results.\"\"\"\n",
    "    # Chunk each row of the DataFrame\n",
    "    chunks = df.map(chunker, batch_size=batch_size, is_batched_fn=batch_size > 1, pbar=True, inputs={chunk_col: 'files'})\n",
    "    df['chunks'] = chunks\n",
    "    \n",
    "    # Make a df on each row, propagate the other columns\n",
    "    chunk_dfs = df.map(\n",
    "        lambda row: mk.DataFrame({\n",
    "            'chunk': row['chunks'], \n",
    "            'chunk_idx': list(range(1, len(row['chunks']) + 1)),\n",
    "            **{k: [v] * len(row['chunks']) for k, v in row.items() if k in ['filename', 'file_id'] }\n",
    "        }), \n",
    "        pbar=True,\n",
    "    )\n",
    "    \n",
    "    # Concatenate the results\n",
    "    return mk.concat(chunk_dfs)\n",
    "\n",
    "def chunker(files: List[str], toksize: int = 2048) -> List[str]:\n",
    "    \"\"\"Split each file into chunks of size toksize.\"\"\"\n",
    "    # Get the encoding\n",
    "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Tokenized files\n",
    "    tokens = encoding.encode_batch(files)\n",
    "    # Split each file into chunks of size toksize\n",
    "    splits = [[encoding.decode(e[pos:pos + toksize]) for pos in range(0, len(e), toksize)] for e in tokens]\n",
    "    return splits\n",
    "\n",
    "chunk_df = explode(project, 'files', partial(chunker, toksize=2048), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://localhost:8014/?id=Table958dc751-376d-4e23-af40-a9ceb01fdbf1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb5b4612ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_df['chunk'] = chunk_df['chunk'].format(mk.format.CodeFormatterGroup())\n",
    "chunk_df.create_primary_key(\"chunk_id\")\n",
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import cohere\n",
    "openai.api_key = \"sk-xxx\"\n",
    "co = cohere.Client(\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"cohere/large\"\n",
    "model = \"openai/text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:07<00:00,  6.13s/it]\n"
     ]
    }
   ],
   "source": [
    "def embed(text, model=\"openai/text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   if model.startswith(\"openai\"):\n",
    "      response = openai.Embedding.create(input = [text], model=model.replace(\"openai/\", \"\"))\n",
    "      return response['data'][0]['embedding']\n",
    "   elif model.startswith(\"cohere\"):\n",
    "      response = co.embed(texts=[text], model=model.replace(\"cohere/\", \"\"))\n",
    "      return response.embeddings[0]\n",
    "\n",
    "def embed_many(texts, model=\"openai/text-embedding-ada-002\"):\n",
    "   texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "   if model.startswith(\"openai\"):\n",
    "      response = openai.Embedding.create(input=texts, model=model.replace(\"openai/\", \"\"))\n",
    "      return [response['data'][i]['embedding'] for i in range(len(texts))]\n",
    "   elif model.startswith(\"cohere\"):\n",
    "      response = co.embed(texts=texts, model=model.replace(\"cohere/\", \"\"))\n",
    "      return response.embeddings\n",
    "\n",
    "# Embed each chunk for retrieval\n",
    "chunk_df[f'embeddings/{model}'] = chunk_df.map(lambda chunk: embed_many(chunk, model=model), pbar=True, batch_size=128, is_batched_fn=True, output_type=mk.TensorColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 129.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add token counts\n",
    "chunk_df['ntokens'] = chunk_df['chunk'].map(get_token_count, batch_size=16, is_batched_fn=True, pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.write('/Users/krandiash/Desktop/workspace/projects/meerkat-dev/mkdev/scratch/karan/chatbot/meerkat-chunks.mk')\n",
    "project.write('/Users/krandiash/Desktop/workspace/projects/meerkat-dev/mkdev/scratch/karan/chatbot/meerkat-project.mk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://localhost:8014/?id=Table1317a606-2a4c-4dbb-9639-20426930bcc3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb5a2632580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(df, query, n=10, embedding_col: str = f'embeddings/{model}'):\n",
    "    # Embed the query\n",
    "    query_embedding = embed(query, model=model)\n",
    "    # Compute the cosine similarity between the query and each chunk\n",
    "    similarities = df[embedding_col].dot(query_embedding)\n",
    "    # Sort the chunks by similarity\n",
    "    df['similarity'] = similarities\n",
    "    df = df.sort('similarity', ascending=False)\n",
    "    # Return the top n results\n",
    "    return df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(text: str, ntokens: int):\n",
    "    \"\"\"Truncate a string to a number of tokens.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = encoding.encode(text)[:ntokens]\n",
    "    print(f\"Truncated to {len(tokens)} tokens\")\n",
    "    return encoding.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(instruction, query, context):\n",
    "    return f\"\"\"\n",
    "{instruction}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Context:\n",
    "{context}\n",
    "\n",
    "Helpful Response:\\\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(df, query, n=2, max_tokens=6144):\n",
    "    # Search for the query\n",
    "    results = search(df, query, n=n)\n",
    "    # Create the prompt\n",
    "    instruction = \"Please provide a helpful response to the following query using the provided context. Your response should be well formatted, and can include code snippets.\"\n",
    "    context = truncate(\"\\n\\n\".join(results['chunk']), max_tokens)\n",
    "    return template(instruction, query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Truncated to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6144</span> tokens\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Truncated to \u001b[1;36m6144\u001b[0m tokens\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"100%\"\n",
       "            src=\"http://localhost:8014/?id=flexcol2dee3867-ad9c-492c-bd4f-a18ce2898ddc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb580658910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How do I create an interface that contains a scatter plot and a table in Python?\"\n",
    "prompt = create_prompt(chunk_df, query, n=8)\n",
    "mk.gui.html.flexcol([\n",
    "    mk.gui.Header(\"Copy the prompt and paste it into GPT-4!\"), \n",
    "    mk.gui.CopyButton(value=prompt),\n",
    "    # mk.gui.Markdown(prompt),\n",
    "], classes=\"items-center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
